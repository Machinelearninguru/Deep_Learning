<!DOCTYPE HTML>
<!--
	Arcana by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<link href="../../../../syntaxhighlighter_3.0.83/styles/shCore.css" rel="stylesheet" type="text/css" />
<link href="../../../../syntaxhighlighter_3.0.83/styles/shThemeDefault.css" rel="stylesheet" type="text/css" />
<link href="../../../../assets/css/shell.css" rel="stylesheet" type="text/css" />
<link href="../../../../assets/css/table.css" rel="stylesheet" type="text/css" />


<head>


    <link rel="icon" type="image/png" href="../../../../_images/logo1.png">
    <title>Train a Convolutional Neural Network as a Classifier</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" href="../../../../assets/Resources/elighterjs/Resources/bootstrap/bootstrap.min.css">
    <link rel="stylesheet" href="../../../../assets/css/main.css" />
    <link rel="stylesheet" href="../../../../assets/css/figure_style.css" />
    <link rel="stylesheet" href="../../../../assets/css/text_style.css" />
    <!-- Favicon !-->
    <link rel="stylesheet" type="text/css" href="../../../../assets/Resources/elighterjs/Build/EnlighterJS.min.css" />
    <style>
        .EnlighterJS {
            width: auto !important;
            overflow-x: scroll !important;
            word-wrap: normal !important;
        }
        
        .EnlighterJS li {
            white-space: pre !important;
        }
    </style>
    <script type="text/javascript" src="../../../../assets/Resources/elighterjs/Resources/MooTools.min.js"></script>
    <script type="text/javascript" src="../../../../assets/Resources/elighterjs/Build/EnlighterJS.min.js"></script>
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} }); </script>
    <script type="text/javascript">
        window.addEvent('domready', function() {
            EnlighterJS.Util.Helper(document.getElements('#pycode pre'), {
                indent: 4,
                language: 'python',
                renderer: 'Block',
                grouping: true,
                theme: 'Beyond',
                showLinenumbers: true
            });
            EnlighterJS.Util.Helper(document.getElements('#pycode code.ih'), {
                renderer: 'Inline',
                language: 'python',
                theme: 'Beyond'
            });
        });
    </script>
</head>

<body>
    <div id="page-wrapper">

        <!-- Header -->
        <div id="header">

            <!-- Nav -->
            <nav id="nav">
                <ul>
                    <li><a href="../../../../index.php">Home</a></li>
                    <li><a href="#">></a></li>
                    <li><a href="../../../../topics.php?my_id=4">Deep Learning</a></li>
                    <li><a href="#">></a></li>
                    <li><a href="../../../../posts.php?my_id=10">TensorFlow</a></li>
                </ul>
            </nav>

        </div>

        <!-- Main -->
        <section class="wrapper style1">
            <div class="container">
                <div class="row">
                    <div class="4u 12u(narrower)">
                        <div id="sidebar">
                            <section>
                                <h3><img src="../../../../_images/logo2.png" style="width:64px;height:64px;vertical-align: middle;" alt="" />&emsp;<a id="logo" style="vertical-align: middle;" href="../../../../index.html">Machine Learning Guru</a></h3>
                                <p>Machine Learning and Computer Vision tutorials using open source packages.</p>
                            </section>
                            <section>
                                <h3>Sections</h3>
                                <ul class="links">
                                    <li><a href="#intro">Introduction</a></li>
                                    <li><a href="#Input Pipeline">Input Pipeline</a></li>
                                    <li><a href="#Network Architecture">Network Architecture</a></li>
                                    <li><a href="#The TensorFlow Graph">The TensorFlow Graph</a></li>
                                    <li><a href="#Training">Training</a></li>
                                    <li><a href="#Training Summaries and Results">Training Summaries and Results</a></li>






                                    <li><a href="#Summary">Summary</a></li>


                                </ul>
                            </section>
                        </div>
                    </div>
                    <div class="8u  12u(narrower) important(narrower)">
                        <div id="content">
                            <!--    #######################             -->
                            <!--    #######################             -->
                            <!--    Content: Your post starts here      -->
                            <!--    #######################             -->
                            <!--    #######################             -->
                            <article id="post_top">
                                <header align="center">
                                    <h2>Train a Convolutional Neural Network as a Classifier</h2>
                                    <p>This tutorial deals with training a classifier using convolutional neural networks.</p>
                                </header>
                                <!--    #######################  ####################### ####################### #######################            -->
                                <!--    #######################  ####################### ####################### #######################            -->
                                <!--    #######################  ####################### ####################### #######################            -->
                                <!--    #######################  ####################### ####################### #######################            -->
                                <h2 id="intro">Introduction</h2>
                                <p align="justify"> In this tutorial we try to teach you how to implement a simple neural network image classifier using <b>Convolutional Neural Networks(CNNs)</b>. The main goal of this post is to show hot to train a CNN classifier using <a href="https://www.tensorflow.org/">TensorFlow</a> deep learning framework developed by Google. The deep learning concepts such as the details of CNNs will not be discussed here. In order to get a better idea of convolutional layers and realize how the work please refer to <a href="http://machinelearninguru.com/computer_vision/basics/convolution/convolution_layer.html">this post</a>. In the next section we start to describe procedure of learning the classifier.

                                </p>

                                <h2 id="Input Pipeline">Input Pipeline</h2>

                                <p align="justify"> The dataset that we work on that in this tutorial is the <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> dataset probably the most famous dataset in computer vision because of its simplicity! The main dataset consist of 60000 training and 10000 test images. However there might be different setups for these images. The one we use is the same in the test set but we split the training set to 55000 images as train and 5000 images as validation set in the case that using cross-validation for determining some hyper-parameters is desired. The images are 28x28x1 which each of them represent a hand-written digit from 0 to 9. Since this tutorial is supposed to be ready-to-use, we provided the code to download and extract the MNIST data as a data object. Thanks to TensorFlow its code is already written and is ready to use and its source code is available at <a href="tensorflow.contrib.learn.python.learn.datasets.mnist">this repository</a> . The code for downloading and extracting MNIST dataset is as is as below:

                                </p>


                                <div class="panel panel-default">
                                    <div class="panel-heading">Download and Extract MNIST dataset</div>
                                    <div class="panel-body" id="pycode">


                                        <pre>
 
from tensorflow.examples.tutorials.mnist import input_data
import tensorflow as tf

mnist = input_data.read_data_sets("MNIST_data/", reshape=False, one_hot=False)

# The 'input.provide_data' is provided to organize any custom dataset.
data = input.provide_data(mnist)

</pre>

                                    </div>
                                </div>

                                <p align="justify">The above code download and extract MNIST data in the <span class="inner_shadow">MNIST_data/</span> folder in the current directory that we are running the python script. The <span class="inner_shadow">reshape flag</span> is set to <b>False</b> because we want the image format as it is which is 28x28x1. The reason is because we are aimed to train a CNN classifier which takes images as input. If the <span class="inner_shadow">one_hot flag</span> is set to <b>True</b> it returns class labels as a one_hot label. However we set the one_hot flag to <b>False</b> for customized preprocessing and data organization. The <b>input.provide_data</b> function is provided to get any data with specific format separated by training and testing sets and return the structured data object for further processing. From now on we consider <b>data</b> as the data object which has the following structure:</p>

                                <!--                                  <html>  -->

                                <head>
                                    <script type='text/javascript' src='https://www.google.com/jsapi'></script>
                                    <script type='text/javascript'>
                                        google.load('visualization', '1', {
                                            packages: ['orgchart']
                                        });
                                        google.setOnLoadCallback(drawChart);

                                        function drawChart() {
                                            var data = new google.visualization.DataTable();
                                            data.addColumn('string', 'Node');
                                            data.addColumn('string', 'Parent');
                                            data.addRows([
                                                ['data', ''],
                                                ['train', 'data'],
                                                ['validation', 'data'],
                                                ['test', 'data'],
                                                ['images', 'train'],
                                                ['labels', 'train'],
                                                ['images', 'validation'],
                                                ['labels', 'validation'],
                                                ['images', 'test'],
                                                ['labels', 'test'],
                                            ]);
                                            var chart = new google.visualization.OrgChart(document.getElementById('chart_div'));
                                            chart.draw(data);
                                        }
                                    </script>
                                </head>

                                <body>
                                    <div id='chart_div'></div>
                                </body>
                                <!-- </html> -->

                                <p align="justify"> In any of the train, validation and test attributes, sub-attributes of images and labels exist. The have just not been depicted for the simplicity of the above chart presentation. As an example if <b>data.train.imege</b> is called its shape is [number_of_training_sample,28,28,1]. It is recommended to play around a little bit with data object to grasp a better idea of how it works and what is its output. The codes are available in the GitHub repository for this post.

                                </p>

                                <h2 id="Network Architecture">Network Architecture</h2>

                                <p align="justify">After explanation of the data input pipeline, Now it's the time to go through the neural network architecture used for this tutorial. The implemented architecture is very similar to <a href="http://yann.lecun.com/exdb/lenet/">LeNet</a> although our architecture is implemented in a fully-convolutional fashion, i.e., there is no fully-connected layer and all fully-connected layers are transform to corresponding convolutional layer. In order to grasp a better idea of how to go from a fully-connected layer to a convolutional one and vice versa please refer to <a href="http://cs231n.github.io/convolutional-networks/">this link</a>. The general architecture schematic is as below:
                                </p>

                                <div align="center">
                                    <div id="figure1" class="responsive" style="padding: 0 6px;height: 20%;width: 40%;">
                                        <div class="img">
                                            <a target="_blank" href="../../../../_images/topics/deep_learning/tensorflow/neural_networks/cnn_classifier/architecture.png"> <img src="../../../../_images/topics/deep_learning/tensorflow/neural_networks/cnn_classifier/architecture.png" alt="image" width="200" height="150"> </a>
                                            <div class="desc"> <b>Figure 1:</b> The general architecture of the network.</div>
                                        </div>
                                    </div>
                                </div>

                                <p align="justify">The image is depicted by <a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard">Tensorboard</a> as a visualization tool for TensorFlow. Later on in this tutorial the way of using Tensorboard and make the most of it will be explained. As it can be seen by the figure, the convolutional layers are followed by pooling layers and the last fully-connected layer is followed by a dropout layer to decrease the overfitting. <em>The dropout will only be applied in the training phase</em>. The code for designing the architecture is as below:

                                </p>


                                <div class="panel panel-default">
                                    <div class="panel-heading">Network Architecture</div>
                                    <div class="panel-body" id="pycode">


                                        <pre data-enlighter-lineoffset="1" data-enlighter-group="code3" data-enlighter-title="Architecture Design">
 
import tensorflow as tf
slim = tf.contrib.slim

def net_architecture(images, num_classes=10, is_training=False,
                     dropout_keep_prob=0.5,
                     spatial_squeeze=True,
                     scope='Net'):

    # Create empty dictionary
    end_points = {}

    with tf.variable_scope(scope, 'Net', [images, num_classes]) as sc:
        end_points_collection = sc.name + '_end_points'

        # Collect outputs for conv2d and max_pool2d.
        with tf.contrib.framework.arg_scope([tf.contrib.layers.conv2d, tf.contrib.layers.max_pool2d], 
        outputs_collections=end_points_collection):
        
            # Layer-1
            net = tf.contrib.layers.conv2d(images, 32, [5,5], scope='conv1')
            net = tf.contrib.layers.max_pool2d(net, [2, 2], 2, scope='pool1')

            # Layer-2
            net = tf.contrib.layers.conv2d(net, 64, [5, 5], scope='conv2')
            net = tf.contrib.layers.max_pool2d(net, [2, 2], 2, scope='pool2')

            # Layer-3
            net = tf.contrib.layers.conv2d(net, 1024, [7, 7], padding='VALID', scope='fc3')
            net = tf.contrib.layers.dropout(net, dropout_keep_prob, is_training=is_training,
                               scope='dropout3')

            # Last layer which is the logits for classes
            logits = tf.contrib.layers.conv2d(net, num_classes, [1, 1], activation_fn=None, scope='fc4')

            # Return the collections as a dictionary
            end_points = slim.utils.convert_collection_to_dict(end_points_collection)

            # Squeeze spatially to eliminate extra dimensions.
            if spatial_squeeze:
                logits = tf.squeeze(logits, [1, 2], name='fc4/squeezed')
                end_points[sc.name + '/fc4'] = logits
            return logits, end_points

</pre>

                                        <pre data-enlighter-lineoffset="43" data-enlighter-group="code3" data-enlighter-title="Setting Default Values">
 
def net_arg_scope(weight_decay=0.0005):
    #Defines the default network argument scope.

    with tf.contrib.framework.arg_scope(
            [tf.contrib.layers.conv2d],
            padding='SAME',
            weights_regularizer=slim.l2_regularizer(weight_decay),
            weights_initializer=tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_AVG',
                                                                               uniform=False, seed=None,
                                                                               dtype=tf.float32),
            activation_fn=tf.nn.relu) as sc:
        return sc

</pre>

                                    </div>
                                </div>

                                <h3 id="Default Parametes and Operations">Default Parameters and Operations</h3>

                                <p align="justify"> The function <span class="inner_shadow">net_arg_scope</span> is defined to share some attributes between layers. It is very useful in the cases which some attributes like 'SAME' padding(which is zero-padding in essense) are joint between different layer. It basically does the sharing variable with some pre-definitions. Basically it enables us to specify different operations and/or a set of arguments to be passed to any of the defined operations in the arg_scope. So for this specific case the argument <b>tf.contrib.layers.conv2d</b> is defined and so all the convolutional layers default parameters(which are ser by the arg_scope) are as defined in the arg_scope. The is more work to use this useful arg_scope operation and it will be explained in the general TensorFlow implementation details later on in this tutorial. It is worth noting that all the parameters defined by arg_scope, can be overwritten locally in the specific layer definition. As an example take a look at <b>'line 28'</b>. While defining the tf.contrib.layers.conv2d layer(the convolutional layer), the padding is set to <b>'VALID'</b> although its default been set to <b>'SAME'</b> by the arg_scope operation. Now it's the time to explain the architecture itself by describing of how to create cnvolutional and pooling layers.</p>

                                <p align="justify">
                                    <span class="inner_shadow">ReLU</span> has been used as the non-linear activation function for all the layers except the last layer(embedding layer). The famous xavier initialization has not been used for initialization of the network and instead the <span class="inner_shadow">Variance-Scaling-Initializer</span> has been used which provided more promising results in the case of using ReLU activation. It's advantage is to keep the scale of the input variance constant, so it is claimed that it does not explode or diminish by getting to the final layer<a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/variance_scaling_initializer">[reference]</a>. There are different types of variance-scaling initializers. The one we used in is the one proposed by the paper <a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks</a> and provided by the TensorFlow. is the one proposed by the paper <a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks</a> and provided by the TensorFlow.
                                </p>


                                <h3 id="Convolution Layers">Convolution and Pooling Layers</h3>

                                <p align="justify">
                                    Now it's the time to build our convolutional architecture using convolution and pooling layers which are defined in the <span class="inner_shadow">net_architecture</span> panel in the above python script. It is worth noting that since the output of layers(output tensors) are different by the size the output sizes decrease gradually as we go through the depth of the network, the matching between inputs-outputs of the layers must be considered and in the end the output of the last layer should be form into a feature vector in order to be fed to the embedding layer.
                                </p>

                                <p align="justify">
                                    Defining pooling layers is straightforward as it is shown in <b>'line 29'</b>. The defined pooling layer has the kernel size of 2x2 and a stride of 2 in each dimension. This is equivalent to extract the maximum in each 2x2 windows and the stride makes no overlapping in the chosen windows for max pooling operation. In order to have a better understanding of pooling layer please refer to <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/max_pool2d">this link</a>.
                                </p>

                                <p align="justify">
                                    Convolution layers can be defined as of written in <b>'line 20'</b> using <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers">tf.contrib.layers</a>. The default padding is set to 'SAME' as mentioned before. loosely speaking, 'SAME' padding equals to same spatial dimensions for output feature map and input feature map which contains zero padding to matching the shapes and theoretically it is done equally on every side of the input map. One the other hand, 'VALID' means no padding. The overall architecture of the convolution layer is as depicted below:
                                </p>

                                <div align="center">
                                    <div id="figure1" class="responsive" style="padding: 0 6px;height: 30%;width: 50%;">
                                        <div class="img">
                                            <a target="_blank" href="../../../../_images/topics/deep_learning/tensorflow/neural_networks/cnn_classifier/convlayer.png"> <img src="../../../../_images/topics/deep_learning/tensorflow/neural_networks/cnn_classifier/convlayer.png" alt="image" width="200" height="150"> </a>
                                            <div class="desc"> <b>Figure 2:</b> The operations in convolutional layer.</div>
                                        </div>
                                    </div>
                                </div>

                                <p align="justify">
                                    Let's get back to <b>'line 20'</b>. The number of <b>output feature maps</b> is set to 32 and the <b>spatial kernel size</b> is set to [5,5]. The <b>stride</b> is [1,1] by default. The <b>scope</b> argument is for defining the name for the layer which is useful in different scenarios such as returning the output of the layer, fine-tuning the network and graphical advantages like drawing a nicer graph of the network using Tensorboard. Basically it is the representative of the layer and adds all the operations into a higher-level node.
                                </p>

                                <p align="justify">
                                    In <b>'line 29'</b> we overwritten the padding type. It is changed to 'VALID' padding. The reason is behind the characteristics of the convolutional layer defined in <b>'line 28'</b>. It is operating as a fully-connected layer. <em>It is not because of the 'VALID' padding though</em>. The 'VALID' padding is just part of the mathematical operation. The reason is because the input to this layer has the spatial size of <b>7x7</b> and the kernel size of the layer is the same. This is obvious because when the input size of the convolutional layer equals to its kernel size and 'VALID' pooling is used, the output is only one single neuron if the number of output feature map equals to 1. So if the number of output feature maps is equals to 1024, this layer operates like and filly-connected layer with 1024 output hidden units!

                                </p>

                                <h3 id="Dropout Layer">Dropout Layer</h3>
                                <p align="justify">
                                    The dropout is one of the most famous methods in order to prevent over-fitting. This operation randomly kills a portion of neuron to stochastically force the neuron to learn more useful information. Although the method is stochastic but it's been widely used in neural network architecture and presented promising results. The layer is defined in <b>line '29'</b>. The <span class="inner_shadow">dropout_keep_prob</span> argument determines the portion of the neurons which remains untouched and will not be disables by the dropout layer. Moreover the flag <span class="inner_shadow">is_training</span> is supposed to active and deactive the dropout layer which force the dropout to be <b>active</b> in the training phase and <b>deactivate</b> it in the test/evaluation phase.
                                </p>

                                <h3 id="Embedding Layer">Embedding Layer</h3>
                                <p align="justify">
                                    Convolutional layers results a 4-dimensional tensor with dimensions as [batch_size, width, height, channel]. As a result, the embedding layer combines all the channels except the first one indicating the batches. So the dimension of [batch_size, width, height, channel] becomes [batch_size, width x height x channel]. Let's go to <b>'line 33'</b>. This is the last fully-connected layer prior to <span class="inner_shadow">softmax</span> which the number of its output units must be equal to the number of classes. The output of this layer has the dimensionality of [batch_size, 1, 1, num_classes]. <b>'Line 39-42'</b> does the embedding operation which its output dimension is [batch_size, num_classes]. It is worth noting that the scope of the last layer overwrite the <span class="inner_shadow">scope='fc4'</span>.
                                </p>


                                <h2 id="The TensorFlow Graph">The TensorFlow Graph</h2>
                                <p align="justify">
                                    At this time, after describing the network design and different layers, it is the time to present how to implement this architecture using TensorFlow. With TensorFlow everything should be defined on something called <span class="inner_shadow">GRAPH</span>. The graphs has the duty to tell the TensorFlow backend to what to do and how to do the desired operations. TensorFlow uses <span class="inner_shadow">Session</span> to run the operations.
                                </p>
                                <p align="justify">
                                    The graph operations are executed in session environment which contains state of variables. For running each created session a specific graph is needed because each session can only be operated on a single graph. So multiple graphs cannot be used in a single session. If the users does not explicitly use a session by its name, the default session will be used by TensorFlow.
                                </p>
                                <p align="justify">
                                    A graph contains tensors and the operations defined on that graph. So the graph can be used on multiple sessions. Again like the sessions, if a graph is not explicitly defined by the user, the TensorFlow itself set a default graph. Although there is no harm working with the default graph, but explicitly defining the graph is recommended. The general graph of out experimental setup is as below:
                                </p>

                                <div align="center">
                                    <div id="figure1" class="responsive" style="padding: 0 6px;height: 40%;width: 60%;">
                                        <div class="img">
                                            <a target="_blank" href="../../../../_images/topics/deep_learning/tensorflow/neural_networks/cnn_classifier/graph.png"> <img src="../../../../_images/topics/deep_learning/tensorflow/neural_networks/cnn_classifier/graph.png" alt="image" width="200" height="150"> </a>
                                            <div class="desc"> <b>Figure 3:</b> The TensorFlow Graph.</div>
                                        </div>
                                    </div>
                                </div>

                                <p align="justify">
                                    The graph is explicitly defined in our experiments. The following script, panel by panel, shows the graph design of our experiments:
                                </p>

                                <div class="panel panel-default">
                                    <div class="panel-heading">TensorFlow Graph Design</div>
                                    <div class="panel-body" id="pycode">


                                        <pre data-enlighter-lineoffset="1" data-enlighter-group="code1" data-enlighter-title="Graph Default">
 
graph = tf.Graph()
with graph.as_default():

</pre>

                                        <pre data-enlighter-lineoffset="3" data-enlighter-group="code1" data-enlighter-title="Parameters">
 

    # global step
    global_step = tf.Variable(0, name="global_step", trainable=False)

    # learning rate policy
    decay_steps = int(num_train_samples / FLAGS.batch_size *
                      FLAGS.num_epochs_per_decay)
    learning_rate = tf.train.exponential_decay(FLAGS.initial_learning_rate,
                                               global_step,
                                               decay_steps,
                                               FLAGS.learning_rate_decay_factor,
                                               staircase=True,
                                               name='exponential_decay_learning_rate')
</pre>

                                        <pre data-enlighter-lineoffset="13" data-enlighter-group="code1" data-enlighter-title="Place Holders">
 
    # Place holders
    image_place = tf.placeholder(tf.float32, shape=([None, height, width, num_channels]), name='image')
    label_place = tf.placeholder(tf.float32, shape=([None, FLAGS.num_classes]), name='gt')
    dropout_param = tf.placeholder(tf.float32)
</pre>
                                        <pre data-enlighter-lineoffset="16" data-enlighter-group="code1" data-enlighter-title="Model and Evalutaion Tensors">
 
    # MODEL
    arg_scope = net.net_arg_scope(weight_decay=0.0005)
    with tf.contrib.framework.arg_scope(arg_scope):
        logits, end_points = net.net_architecture(image_place, num_classes=FLAGS.num_classes, dropout_keep_prob=dropout_param,
                                       is_training=FLAGS.is_training)

    # Define loss
    with tf.name_scope('loss'):
        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=label_place))

    # Accuracy
    with tf.name_scope('accuracy'):
        # Evaluate model
        correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(label_place, 1))

        # Accuracy calculation
        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
</pre>

                                        <pre data-enlighter-lineoffset="33" data-enlighter-group="code1" data-enlighter-title="Training Tensors">
 
    # Define optimizer by its default values
    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)

    # Gradient update.
    with tf.name_scope('train'):
        grads_and_vars = optimizer.compute_gradients(loss)
        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)
</pre>

                                        <pre data-enlighter-lineoffset="33" data-enlighter-group="code1" data-enlighter-title="Summaries">
 
    arr = np.random.randint(data.train.images.shape[0], size=(3,))
    tf.summary.image('images', data.train.images[arr], max_outputs=3,
                     collections=['per_epoch_train'])

    # Histogram and scalar summaries sammaries
    for end_point in end_points:
        x = end_points[end_point]
        tf.summary.scalar('sparsity/' + end_point,
                          tf.nn.zero_fraction(x), collections=['train', 'test'])
        tf.summary.histogram('activations/' + end_point, x, collections=['per_epoch_train'])

    # Summaries for loss, accuracy, global step and learning rate.
    tf.summary.scalar("loss", loss, collections=['train', 'test'])
    tf.summary.scalar("accuracy", accuracy, collections=['train', 'test'])
    tf.summary.scalar("global_step", global_step, collections=['train'])
    tf.summary.scalar("learning_rate", learning_rate, collections=['train'])

    # Merge all summaries together.
    summary_train_op = tf.summary.merge_all('train')
    summary_test_op = tf.summary.merge_all('test')
    summary_epoch_train_op = tf.summary.merge_all('per_epoch_train')
</pre>

                                    </div>
                                </div>
                                <p align="justify">
                                    Each of the above panels, will be explained in the following subsections using the same naming convention for convenience.
                                </p>

                                <h3 id="Graph Default">Graph Default</h3>

                                <p align="justify">
                                    As mentioned before, it is recommended to set the graph manually and in that section, we named the graph to be <b>graph</b>. Later on it will be notice that this definition is useful because we can pass the graph to other functions and sessions and it will be recognized.
                                </p>


                                <h3 id="Parameters">Parameters</h3>

                                <p align="justify">
                                    Different parameters are necessary for the learning procedure. The <span class="inner_shadow">global_step</span> defined in <b>'line 4'</b> is one of which. The reason behind defining the global_step is to have a track of where we are in the training procedure. It is a non-learnable tensor and should be incremented per each gradient update which will be done over each batch. The <span class="inner_shadow">decay_steps</span> defined in <b>'line 7'</b> determines after how many steps or epochs the learning rate should be decreased by a predefined policy. As can be seen <b>num_epochs_per_decay</b> defines the decay factor which is restricted to the number of passed epochs. The <span class="inner_shadow">learning_rate</span> tensor defined in <b>'line 9'</b> determines the learning rate policy. Please refer to TensorFlow official documentation for grasping a better idea of the <em>tf.train.exponential_decay</em> layer. It is worth noting that the <em>tf.train.exponential_decay</em> layer takes <em>global_step</em> as its counter to realize when it has to change the learning rate.

                                </p>


                                <h3 id="Place Holders">Place Holders</h3>

                                <p align="justify">
                                    The <span class="inner_shadow">tf.placeholder</span> operation, creates a placeholder variable tensor which will be fed to the network in testing/training phase. The images and labels must have placeholders because they are in essence the inputs to the network in training/testing phase. The <em>type</em> and <em>shape</em> of the place holders must be defined as required parameters. As can be seen in <b>line '14'</b>, the first dimension of the shape argument is set to <b>None</b> which allows the place holder to get any dimension. The first dimension is the <em>batch_size</em> and is flexible.
                                </p>
                                <p align="justify">
                                    The <span class="inner_shadow">dropout_param</span> placeholder, takes the probability of keeping a neuron active. The reason behind defining a placeholder for dropout parameter is to enable the setup to take this parameter in running each any session arbitrary which enrich the experiment to disable it when running the testing session.

                                </p>

                                <h3 id="Model and Evalutaion Tensors">Model and Evaluation Tensors</h3>
                                <p align="justify">
                                    The <b>'lines 17-20'</b> provides the default parameters determines by <b>arg_scope</b> operator. The <em>tf.nn.softmax_cross_entropy_with_logits</em> on the un-normalized logits is used as the loss function. This function computes the softmax activation internally which makes it more stable. Finally in <b>'lines 27-32'</b> the accuracy is computed.
                                </p>

                                <h3 id="Training Tensors">Training Tensors</h3>
                                <p align="justify">
                                    Now it's the time to define the training tensors. As defined in <b>'line 34'</b> the <span class="inner_shadow">Adam Optimizer</span> is used as one of the best current optimization algorithms which is widely used and is famous because of its adaptive characteristics. As is defined in <b>'lines 37-39'</b>, the gradients must be computed using the <em>defined loss tensor</em> and those computations must be added as the <em>train operations</em> to the graph. Basically <span class="inner_shadow">'train_op'</span> is an operation that is run for gradient update on parameters. Each execution of 'train_op' is a training step. By passing <span class="inner_shadow">'global_step'</span> to the optimizer, each time that the 'train_op' is run, TensorFlow update the 'global_step' and increment it by one!
                                </p>


                                <h3 id="Summaries">Summaries</h3>

                                <p align="justify">
                                    In this section we describe how to create summary operations and save them into allocated tensors. Eventually the summaries should be presented in <em>Tensorboard</em> in order to visualize what is happening inside of the network blackbox. There are different types of summaries. Three type of <span class="inner_shadow">image</span>, <span class="inner_shadow">scalar</span> and <span class="inner_shadow">histogram</span> summaries are used in this implementations. In order to avoid this post to becoming too verbose, we do not go in depth of the explanation for summary operations and we will get back to it in another post.
                                </p>

                                <p align="justify">

                                    In <b>'lines 33-35'</b> image summaries are created which has the duty of visualize the input elements to the summary tensor. This elements here are 3 random images from the train data. In <b>'lines 38-42'</b> the outputs of different layers will be fed to the relevent summary tensor. Finally in <b>'lines 45-48'</b> some scalar summaries are created in order to track the <em>training convergence</em> and <em>testing performance</em>. The <span class="inner_shadow">collections</span> argument in summary definitions is a supervisor which direct each summary tensor to the relevent operation. For example some summaries only needs to be generated in training phase and some are only needed in testing. We have a collection named 'per_epoch_train' too and the summaries which only have to be generated once per epoch in the training phase, will be stored in this list. Eventually <b>'lines 51-53'</b> are defined with the goal of gathering the summaries in the corresponding summary lists using the <span class="inner_shadow">collections key</span>.
                                </p>


                                <h2 id="Training">Training</h2>

                                <p align="justify">
                                    Now it's the time to go through the training procedure. In consists of different steps which starts by <b>session configuration</b> to saving the <b>model checkpoint</b>.
                                </p>

                                <h3 id="Configuration and Initialization">Configuration and Initialization</h3>

                                <p align="justify">
                                    First of all the tensors should be gathered for convenience and the session must be configured. The code is as below:
                                </p>


                                <div class="panel panel-default">
                                    <div class="panel-heading">Session Configuration</div>
                                    <div class="panel-body" id="pycode">


                                        <pre data-enlighter-lineoffset="54" data-enlighter-title="Summaries">
 
tensors_key = ['cost', 'accuracy', 'train_op', 'global_step', 'image_place', 'label_place', 'dropout_param',
                   'summary_train_op', 'summary_test_op', 'summary_epoch_train_op']
tensors = [loss, accuracy, train_op, global_step, image_place, label_place, dropout_param, summary_train_op,
               summary_test_op, summary_epoch_train_op]
tensors_dictionary = dict(zip(tensors_key, tensors))

# Configuration of the session
session_conf = tf.ConfigProto(
    allow_soft_placement=FLAGS.allow_soft_placement,
    log_device_placement=FLAGS.log_device_placement)
sess = tf.Session(graph=graph, config=session_conf)
</pre>

                                    </div>
                                </div>

                                <p align="justify">
                                    As it is clear from <b>'line 58'</b>, all the tensors are store in a dictionary to be used later by the corresponding keys. <b>'Line 61-63'</b> are dedicated to configuring the network. The <span class="inner_shadow">allow_soft_placement</span> flag, allows the switching back-and-forth between different devices. This is useful when the user allocated 'GPU' to all operations without considering the fact that not all operations are supported by GPU using the TensorFlow. In this case if the <em>allow_soft_placement</em> operator is disabled, errors can be show up and the user must start the debugging process but using the active flag prevent this issue by automatically switch from a non-supported device to the supported one. The <span class="inner_shadow">log_device_placement</span> flag is to present which operations are set on what devices. This is useful for debugging and it projects a verbose dialog in the terminal. Eventually in <b>'line 64'</b> the session is taken using the defined <b>graph</b>. The training phase start using the following script:
                                </p>

                                <div class="panel panel-default">
                                    <div class="panel-heading">Trainig Operations</div>
                                    <div class="panel-body" id="pycode">


                                        <pre data-enlighter-lineoffset="65" data-enlighter-title="train">
 
with sess.as_default():
    # Run the saver.
    # 'max_to_keep' flag determines the maximum number of models that the tensorflow save and keep. default by TensorFlow = 5.
    saver = tf.train.Saver(max_to_keep=FLAGS.max_num_checkpoint)

    # Initialize all variables
    sess.run(tf.global_variables_initializer())

    ###################################################
    ############ Training / Evaluation ###############
    ###################################################
    train_evaluation.train(sess, saver, tensors_dictionary, data,
                             train_dir=FLAGS.train_dir,
                             finetuning=FLAGS.fine_tuning,
                             num_epochs=FLAGS.num_epochs, checkpoint_dir=FLAGS.checkpoint_dir,
                             batch_size=FLAGS.batch_size)
                                 
    train_evaluation.evaluation(sess, saver, tensors_dictionary, data,
                           checkpoint_dir=FLAGS.checkpoint_dir)
</pre>

                                    </div>
                                </div>


                                <p align="justify">
                                    In <b>'line 68'</b> the <span class="inner_shadow">tf.train.Saver</span> is run in order to provide an operation to save and load the models. The <b>max_to_keep</b> flags determines the maximum number of the saved models that the TensorFlow keeps and its default is set to '5' by TensorFlow. In <b>'line 73'</b> the session is run in order to initialize all the variable which is necessary. Finally in <b>'line 76'</b> <span class="inner_shadow">train_evaluation</span> function is provided to run the training/tesing phase.
                                </p>

                                <h3 id="Training Function">Training Operations</h3>

                                The training function is as below:
                                <div class="panel panel-default">
                                    <div class="panel-heading">Training Function</div>
                                    <div class="panel-body" id="pycode">


                                        <pre data-enlighter-lineoffset="1" data-enlighter-title="train">
 
from __future__ import print_function
import tensorflow as tf
import numpy as np
import progress_bar
import os
import sys

def train(sess, saver, tensors, data, train_dir, finetuning,
                num_epochs, checkpoint_dir, batch_size):
    """
    This function run the session whether in training or evaluation mode.
    :param sess: The default session.
    :param saver: The saver operator to save and load the model weights.
    :param tensors: The tensors dictionary defined by the graph.
    :param data: The data structure.
    :param train_dir: The training dir which is a reference for saving the logs and model checkpoints.
    :param finetuning: If fine tuning should be done or random initialization is needed.
    :param num_epochs: Number of epochs for training.
    :param checkpoint_dir: The directory of the checkpoints.
    :param batch_size: The training batch size.

    :return:
             Run the session.
    """

    # The prefix for checkpoint files
    checkpoint_prefix = 'model'

    ###################################################################
    ########## Defining the summary writers for train /test ###########
    ###################################################################

    train_summary_dir = os.path.join(train_dir, "summaries", "train")
    train_summary_writer = tf.summary.FileWriter(train_summary_dir)
    train_summary_writer.add_graph(sess.graph)

    test_summary_dir = os.path.join(train_dir, "summaries", "test")
    test_summary_writer = tf.summary.FileWriter(test_summary_dir)
    test_summary_writer.add_graph(sess.graph)

    # If fie-tuning flag in 'True' the model will be restored.
    if finetuning:
        saver.restore(sess, os.path.join(checkpoint_dir, checkpoint_prefix))
        print("Model restored for fine-tuning...")

    ###################################################################
    ########## Run the training and loop over the batches #############
    ###################################################################
    for epoch in range(num_epochs):
        total_batch_training = int(data.train.images.shape[0] / batch_size)

        # go through the batches
        for batch_num in range(total_batch_training):
            #################################################
            ########## Get the training batches #############
            #################################################

            start_idx = batch_num * batch_size
            end_idx = (batch_num + 1) * batch_size

            # Fit training using batch data
            train_batch_data, train_batch_label = data.train.images[start_idx:end_idx], data.train.labels[
                                                                                        start_idx:end_idx]

            ########################################
            ########## Run the session #############
            ########################################

            # Run optimization op (backprop) and Calculate batch loss and accuracy
            # When the tensor tensors['global_step'] is evaluated, it will be incremented by one.
            batch_loss, _, train_summaries, training_step = sess.run(
                [tensors['cost'], tensors['train_op'], tensors['summary_train_op'],
                 tensors['global_step']],
                feed_dict={tensors['image_place']: train_batch_data,
                           tensors['label_place']: train_batch_label,
                           tensors['dropout_param']: 0.5})

            ########################################
            ########## Write summaries #############
            ########################################

            # Write the summaries
            train_summary_writer.add_summary(train_summaries, global_step=training_step)

            # # Write the specific summaries for training phase.
            # train_summary_writer.add_summary(train_image_summary, global_step=training_step)

            #################################################
            ########## Plot the progressive bar #############
            #################################################

            progress = float(batch_num + 1) / total_batch_training
            progress_bar.print_progress(progress, epoch_num=epoch + 1, loss=batch_loss)

        # ################################################################
        # ############ Summaries per epoch of training ###################
        # ################################################################
        train_epoch_summaries = sess.run(tensors['summary_epoch_train_op'],
                                         feed_dict={tensors['image_place']: train_batch_data,
                                                    tensors['label_place']: train_batch_label,
                                                    tensors['dropout_param']: 0.5})

        # Put the summaries to the train summary writer.
        train_summary_writer.add_summary(train_epoch_summaries, global_step=training_step)

        #####################################################
        ########## Evaluation on the test data #############
        #####################################################

        # WARNING: In this evaluation the whole test data is fed. In case the test data is huge this implementation
        #          may lead to memory error. In presence of large testing samples, batch evaluation on testing is
        #          recommended as in the training phase.
        test_accuracy_epoch, test_summaries = sess.run([tensors['accuracy'], tensors['summary_test_op']],
                                                       feed_dict={tensors['image_place']: data.test.images,
                                                                  tensors[
                                                                      'label_place']: data.test.labels,
                                                                  tensors[
                                                                      'dropout_param']: 1.})
        print("Epoch " + str(epoch + 1) + ", Testing Accuracy= " + \
              "{:.5f}".format(test_accuracy_epoch))

        ###########################################################
        ########## Write the summaries for test phase #############
        ###########################################################

        # Returning the value of global_step if necessary
        current_step = tf.train.global_step(sess, tensors['global_step'])

        # Add the counter of global step for proper scaling between train and test summaries.
        test_summary_writer.add_summary(test_summaries, global_step=current_step)

    ###########################################################
    ############ Saving the model checkpoint ##################
    ###########################################################

    # # The model will be saved when the training is done.

    # Create the path for saving the checkpoints.
    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)

    # save the model
    save_path = saver.save(sess, os.path.join(checkpoint_dir, checkpoint_prefix))
    print("Model saved in file: %s" % save_path)


    ############################################################################
    ########## Run the session for pur evaluation on the test data #############
    ############################################################################
def evaluation(sess, saver, tensors, data, checkpoint_dir):

        # The prefix for checkpoint files
        checkpoint_prefix = 'model'

        # Restoring the saved weights.
        saver.restore(sess, os.path.join(checkpoint_dir, checkpoint_prefix))
        print("Model restored...")

        # Evaluation of the model
        test_accuracy = 100 * sess.run(tensors['accuracy'], feed_dict={tensors['image_place']: data.test.images,
                                                                       tensors[
                                                                           'label_place']: data.test.labels,
                                                                       tensors[
                                                                           'dropout_param']: 1.})

        print("Final Test Accuracy is %% %.2f" % test_accuracy)


</pre>

                                    </div>
                                </div>

                                <p align="justify">
                                    The input parameters to the function are described by the comments in <b>'lines 12-20'</b>. In <b>'lines 33-39'</b> the summary writers are defined separately for train and test phases. In <b>'lines 45-47'</b> the program checks if <span class="inner_shadow">fine-tuning</span> is desired then the model is loaded and the operation will be continued afterward. In <b>'lines 58-63'</b> The batches are extracted from training data. In <b>'lines 71-76'</b> for a single training step, the model is evaluated on a batch of data and the model parameter and weights will be updated. Same applies for the test set starting in <b>'line 113'</b> however at this time only evaluation will be done and the model won't be updated. In <b>'line 143'</b> the model will be saved.
                                </p>

                                <h2 id="Training Summaries and Results">Training Summaries and Results</h2>
                                <p align="justify">
                                    The training loops saves the summaries in the train summary part. By using the Tensorboard and pointing to the directory that the logs are saved, we can visualize the training procedure. The loss and accuracy for the train are depicted jointly as below:
                                </p>

                                <div align="center">
                                    <div id="figure1" class="responsive" style="padding: 0 6px;height: 80%;width: 100%;">
                                        <div class="img">
                                            <a target="_blank" href="../../../../_images/topics/deep_learning/tensorflow/neural_networks/cnn_classifier/loss_accuracy_train.png"> <img src="../../../../_images/topics/deep_learning/tensorflow/neural_networks/cnn_classifier/loss_accuracy_train.png" alt="image" width="400" height="300"> </a>
                                            <div class="desc"> <b>Figure 4:</b> The loss and accuracy curves for training.</div>
                                        </div>
                                    </div>
                                </div>

                                The <span class="inner_shadow">activation</span> of the last fully-connected layer will be depicted in the following figure:
                                <div align="center">
                                    <div id="figure1" class="responsive" style="padding: 0 6px;height: 50%;width: 50%;">
                                        <div class="img">
                                            <a target="_blank" href="../../../../_images/topics/deep_learning/tensorflow/neural_networks/cnn_classifier/activation_fc4_train.png"> <img src="../../../../_images/topics/deep_learning/tensorflow/neural_networks/cnn_classifier/activation_fc4_train.png" alt="image" width="400" height="300"> </a>
                                            <div class="desc"> <b>Figure 5:</b> The activation of the last layer.</div>
                                        </div>
                                    </div>
                                </div>
                                <p align="justify">
                                    For the last layer it is good to have a visualization of the distribution of the neurons outputs. By using the <span class="inner_shadow">histogram</span> summary the distribution can be shown over the whole training steps. The result is as below:
                                </p>
                                <div align="center">
                                    <div id="figure1" class="responsive" style="padding: 0 6px;height: 50%;width: 50%;">
                                        <div class="img">
                                            <a target="_blank" href="../../../../_images/topics/deep_learning/tensorflow/neural_networks/cnn_classifier/histogram_fc4_train.png"> <img src="../../../../_images/topics/deep_learning/tensorflow/neural_networks/cnn_classifier/histogram_fc4_train.png" alt="image" width="400" height="300"> </a>
                                            <div class="desc"> <b>Figure 6:</b> The histogram summary of the last layer.</div>
                                        </div>
                                    </div>
                                </div>

                                Eventually the test accuracy per step is plotted as the following curve:
                                <div align="center">
                                    <div id="figure1" class="responsive" style="padding: 0 6px;height: 50%;width: 50%;">
                                        <div class="img">
                                            <a target="_blank" href="../../../../_images/topics/deep_learning/tensorflow/neural_networks/cnn_classifier/test_accuracy.png"> <img src="../../../../_images/topics/deep_learning/tensorflow/neural_networks/cnn_classifier/test_accuracy.png" alt="image" width="400" height="300"> </a>
                                            <div class="desc"> <b>Figure 7:</b> Test Accuracy.</div>
                                        </div>
                                    </div>
                                </div>

                                A representation of the terminal progressive bar for the training phase is as below:

                                <div align="center">
                                    <div id="figure1" class="responsive" style="padding: 0 6px;height: 50%;width: 60%;">
                                        <div class="img">
                                            <a target="_blank" href="../../../../_images/topics/deep_learning/tensorflow/neural_networks/cnn_classifier/terminal_training.png"> <img src="../../../../_images/topics/deep_learning/tensorflow/neural_networks/cnn_classifier/terminal_training.png" alt="image" width="400" height="300"> </a>
                                            <div class="desc"> <b>Figure 8:</b> Terminal scene in training phase.</div>
                                        </div>
                                    </div>
                                </div>

                                Few things needs to be considered in order to clarify the results:

                                <ul style="list-style-type:disc">
                                    <li>The initial learning rate by the <b>Adam optimizer</b> has been set to a small number. By setting that to a larger number, the speech of accuracy increasing could go higher. We deliberately set that to a small number to be able to track the procedure easier.</li>
                                    <li>The <b>histogram summaries</b> are saved per each epoch and not per step. Since the generation of histogram summaries are very time-consuming, there are only generated per epoch of training.</li>
                                    <li>While the training is under process, per each epoch an evaluation will be performed over the whole test set. If the test set is too big, isolated evaluation is recommended in order to avoid the memory exhaustion issue.</li>
                                </ul>

                                <h2 id="Summary">Summary</h2>
                                <p align="justify">
                                    In this tutorial we train a neural network classifier using convolutional neural networks. MNIST data has been used for simplicity and its wide usage. The TensorFlow has been used as the deep learning framework. The main goal of this tutorial was to present an easy ready-to-use implementation of training classifiers using TensorFLow. Lots of the tutorials in this category looks like to be too verbose in code or too short in explanations. My effort was to provide a tutorial to be easily understandable in the sense of coding and be comprehensive in the sense of description. Some of the details about some TensorFlow(like summaries) and data-input-pipeline have been ignored for simplicity. We get back to them in the future posts. I hope you enjoyed it.
                                </p>




                                <!--
                                <h2 id="Code Execution">Code Execution</h2> In order to run the python file we go to the terminal and execute the following:
                                <div class="shell-wrap" style="width: 800px">
                                    <p class="shell-top-bar">Build Essentials</p>
                                    <ul class="shell-body">
                                        <li>python /absolute/path/to/python/file </li>
                                    </ul>
                                </div>
-->







                                <!--
                                <p align="justify"><b>FFmpeg</b> is one of the most famous multimedia frameworks wich is widely used for processing videos. In order to encode the video, certainly a video encoder must be used. The popular <span class="inner_shadow">x264</span> is the one which is widely used however it is not super fast! The latest <span class="inner_shadow">NVIDIA GPUs</span> contain a hardware-based video encoder called <span class="inner_shadow">NVENC</span> which is much faster than traditional ones. In order to be able to utilize this gpu-accelerated encoder, FFmpeg must be installed with NVENC support. The full documentation of
                                    FFmpeg integrated with NVIDIA can be fount at <a href="https://developer.nvidia.com/ffmpeg">here</a>. documentation on NVENC can be found <a href="https://developer.nvidia.com/nvidia-video-codec-sdk#NVENCFeatures">here</a>.
                                    Moreover The NVENC programming guide can be found <a href="https://developer.nvidia.com/nvidia-video-codec-sdk#NVENCFeatures">here</a>.

                                </p>
-->






                                <!--
                                <div align="center">
                                    <div id="figure1" class="responsive" style="padding: 0 6px;width: 80%;">
                                        <div class="img">
                                            <a target="_blank" href="../../../_images/topics/computer_vision/video_processing/ffmpeg_rotate_video/txtfileformat.png"> <img src="../../../_images/topics/computer_vision/video_processing/ffmpeg_rotate_video/txtfileformat.png" alt="Linkedin" width="600" height="400"> </a>
                                            <div class="desc"> <b>Figure 1:</b> The format of .txt file.</div>
                                        </div>
                                    </div>
                                </div>
-->


                                <!--
                                <div class="panel panel-default">
                                    <div class="panel-heading">Resizing a video using FFmpeg with NVENC encoder</div>
                                    <div class="panel-body" id="pycode">

                                        
                                        <pre data-enlighter-group="code3" data-enlighter-title="video custome resize">
 
import subprocess
import os
import sys

# Pre...
textfile_path = 'videos.txt'
output_dir_base = 'PATH/TO/OUTPUT'
# Read the text file
with open(textfile_path) as f:
    content = f.readlines()
# you may also want to remove whitespace characters like `\n` at the end of each line
files_list = [x.strip() for x in content]

# Transpose 90 degree & Clockwise
# It already save the video file using the named defined by output_name.
for file_num, file_path_input in enumerate(files_list, start=1):
    # Get the file name without extension
    file_name = os.path.basename(file_path_input)
    ID = file_name.split('_')[1]
    raw_file_name = os.path.basename(file_name).split('.')[0]
    file_dir_input = os.path.dirname(file_path_input)
    file_dir_output = output_dir_base + '/' + ID
    if not os.path.exists(file_dir_output):
        os.makedirs(file_dir_output)
    file_path_output = file_dir_output + '/' + raw_file_name + '.mkv'
    print('processing file: %s' % file_path_input)

    subprocess.call(
        ['ffmpeg', '-y', '-i', file_path_input, '-filter_complex', 
        'nvresize=1:s=540x960:readback=0[out0]', '-map', '[out0]',
         '-acodec', 'copy', '-r', '30', '-vcodec', 'nvenc', '-b:v', '3M', file_path_output])
print('file %s saved' % file_path_output)
</pre>
                                        
                                    </div>
                                </div>
-->




                                <!--
                                <h2 id="Code Execution">Code Execution</h2> In order to run the python file we go to the terminal and execute the following:
                                <div class="shell-wrap" style="width: 800px">
                                    <p class="shell-top-bar">Build Essentials</p>
                                    <ul class="shell-body">
                                        <li>python /absolute/path/to/python/file </li>
                                    </ul>
                                </div>
-->










                                <!--    #######################  ####################### ####################### #######################            -->
                                <!--    #######################  ####################### ####################### #######################            -->
                                <!--    #######################  ####################### ####################### #######################            -->
                                <!--    #######################  ####################### ####################### #######################            -->








                            </article> <a href="#post_top">Go Top</a>
                            <!--    #######################  ####################### ####################### #######################            -->
                            <!--    #######################  ####################### ####################### #######################            -->
                            <!--    #######################  ####################### ####################### #######################            -->
                            <!--    #######################  ####################### ####################### #######################            -->
                            <!--    #######################             -->
                            <!--             Comments                   -->
                            <div id="disqus_thread"></div>
                            <script>
                                (function() { // DON'T EDIT BELOW THIS LINE
                                    var d = document,
                                        s = d.createElement('script');
                                    s.src = '//machine-learning-guru.disqus.com/embed.js';
                                    s.setAttribute('data-timestamp', +new Date());
                                    (d.head || d.body).appendChild(s);
                                })();
                            </script>
                            <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <!-- Footer -->
        <div id="footer">
            <div class="container">
                <div class="row">
                    <section class="3u 6u(narrower) 12u$(mobilep)">
                        <h3>Related Posts:</h3>
                        <!-- <ul class="links">
                            <li><a href="#">Mattis et quis rutrum</a></li>
                            <li><a href="#">Suspendisse amet varius</a></li>
                            <li><a href="#">Sed et dapibus quis</a></li>
                        </ul>
                    </section>
                    <section class="3u 6u$(narrower) 12u$(mobilep)">
                        <h3>More Links to Stuff</h3>
                        <ul class="links">
                            <li><a href="#">Duis neque nisi dapibus</a></li>
                            <li><a href="#">Sed et dapibus quis</a></li>
                            <li><a href="#">Rutrum accumsan sed</a></li>
                        </ul>
                    </section>-->
                </div>
            </div>
            <!-- Icons -->
            <ul class="icons">
                <li><a href="https://twitter.com/M_L_Guru" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
                <li><a href="https://github.com/Machinelearninguru" class="icon fa-github"><span class="label">GitHub</span></a></li>
                <li><a href="https://www.linkedin.com/groups/12030461" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
            </ul>
            <!-- Copyright -->
            <div class="copyright">
                <ul class="menu">
                    <li>&copy; Machine Learning Guru. All rights reserved</li>
                    <li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
                </ul>
            </div>
        </div>
    </div>

    </div>
    <!-- Scripts -->
    <script src="../../../assets/js/jquery.min.js"></script>
    <script src="../../../assets/js/jquery.dropotron.min.js"></script>
    <script src="../../../assets/js/skel.min.js"></script>
    <script src="../../../assets/js/util.js"></script>
    <script src="../../../assets/js/main.js"></script>
    <script id="dsq-count-scr" src="//machine-learning-guru.disqus.com/count.js" async></script>
</body>

</html>